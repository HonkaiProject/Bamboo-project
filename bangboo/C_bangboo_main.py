import json
import cv2
import torch
from transformers import GPT2LMHeadModel, GPT2Tokenizer
from collections import deque
import os

# ë¡œì»¬ GPT-2 ëª¨ë¸ ë¡œë“œ
tokenizer = GPT2Tokenizer.from_pretrained("./gpt2_local")
model = GPT2LMHeadModel.from_pretrained("./gpt2_local")

# YOLOv5 ëª¨ë¸ ë¡œë“œ (Pre-trained)
device = "cuda" if torch.cuda.is_available() else "cpu"
model_yolo = torch.hub.load("ultralytics/yolov5", "yolov5s", pretrained=True).to(device)

# Bangbooì˜ ê¸°ì–µ ì‹œìŠ¤í…œ
MEMORY_FILE = "bangboo_memory.json"
CONVERSATION_HISTORY_FILE = "conversation_history.json"
short_term_memory = deque(maxlen=200)  # ìµœê·¼ ëŒ€í™” ìµœëŒ€ 200ê°œ ê¸°ì–µ

# íŒŒì¼ ì´ˆê¸°í™” ë° ê´€ë¦¬
def initialize_files():
    if not os.path.exists(MEMORY_FILE):
        with open(MEMORY_FILE, "w") as f:
            json.dump({}, f)
    if not os.path.exists(CONVERSATION_HISTORY_FILE):
        with open(CONVERSATION_HISTORY_FILE, "w") as f:
            json.dump([], f)

def load_memory():
    try:
        with open(MEMORY_FILE, "r") as f:
            return json.load(f)
    except FileNotFoundError:
        return {}

def save_memory(memory):
    with open(MEMORY_FILE, "w") as f:
        json.dump(memory, f, indent=4)

memory = load_memory()

def load_conversation_history():
    try:
        with open(CONVERSATION_HISTORY_FILE, "r") as f:
            history = json.load(f)
            return deque(history, maxlen=200)
    except FileNotFoundError:
        return deque(maxlen=200)

def save_conversation_history(history):
    with open(CONVERSATION_HISTORY_FILE, "w") as f:
        json.dump(list(history), f, indent=4)

short_term_memory = load_conversation_history()

# Bangbooì˜ ëŒ€ë‹µ ìƒì„± í•¨ìˆ˜
def generate_bangboo_reply(context):
    input_ids = tokenizer.encode(context, return_tensors="pt")
    output = model.generate(
        input_ids,
        max_length=150,
        num_return_sequences=1,
        no_repeat_ngram_size=2,
        top_k=50,
        top_p=0.9,
        temperature=0.8
    )
    reply = tokenizer.decode(output[0], skip_special_tokens=True).strip()
    return reply

# Bangbooì˜ ì›…ë‚˜ ëŒ€ë‹µ ìƒì„± í•¨ìˆ˜
def generate_woongna_response(user_input):
    context = "\n".join(short_term_memory) + f"\nUser: {user_input}\nBangboo:"
    gpt2_reply = generate_bangboo_reply(context)
    
    # ëŒ€ë‹µ ë ê¸°í˜¸ ì„¤ì •
    if "?" in gpt2_reply:
        symbol = "?!"
    elif "!" in gpt2_reply:
        symbol = "!!"
    else:
        symbol = "."

    response_length = len(user_input)
    woongna = "ì›…" + "ë‚˜" * (response_length // 3 + 2)

    # "ë‚˜"ê°€ 3ê°œ ì´ìƒì´ë©´ ë‹¤ì‹œ "ì›…ë‚˜"ë¡œ ë³€í™˜
    if woongna.count("ë‚˜") > 3:
        woongna_list = ["ì›…ë‚˜" for _ in range((response_length // 6) + 1)]
        woongna = " ".join(woongna_list)

    final_response = f"{woongna}{symbol} ({gpt2_reply})"
    return final_response

# Bangbooì˜ ëŒ€í™” ë¡œì§
def bangboo_reply(user_id, user_input):
    short_term_memory.append(f"User: {user_input}")

    user_name = memory.get("name", "ì¹œêµ¬")
    if user_name is None and "ì´ë¦„" in user_input:
        user_name = user_input.split("ì´ë¦„ì€ ")[-1].strip()
        memory["name"] = user_name
        save_memory(memory)
        return f"ë°˜ê°€ì›Œìš”, {user_name}! ì´ì œ ì´ë¦„ì„ ê¸°ì–µí• ê²Œìš”!"
    
    woongna_response = generate_woongna_response(user_input)
    short_term_memory.append(f"Bangboo: {woongna_response}")
    return woongna_response

# YOLO ê°ì²´ íƒì§€
def describe_scene(frame):
    results = model_yolo(frame)
    detected_objects = results.pandas().xyxy[0]["name"].tolist()
    return detected_objects

def capture_camera():
    """ì¹´ë©”ë¼ ì‹¤í–‰ ë° ê°ì²´ íƒì§€."""
    cap = cv2.VideoCapture(0)
    if not cap.isOpened():
        print("Bangboo: ì¹´ë©”ë¼ê°€ ì—†ìŠµë‹ˆë‹¤. ëŒ€í™” ê¸°ëŠ¥ë§Œ í™œì„±í™”ë©ë‹ˆë‹¤.")
        return False

    print("Bangboo: ì¹´ë©”ë¼ê°€ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤. 'q'ë¥¼ ëˆŒëŸ¬ ì¢…ë£Œí•˜ì„¸ìš”.")

    while True:
        ret, frame = cap.read()
        if not ret:
            print("Bangboo: ì¹´ë©”ë¼ì—ì„œ í”„ë ˆì„ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            break

        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        detected_objects = describe_scene(frame_rgb)

        print(f"Bangboo: ì£¼ë³€ í™˜ê²½ì—ì„œ {', '.join(detected_objects)}ì´(ê°€) íƒì§€ë˜ì—ˆì–´ìš”.")
        cv2.imshow("Bangboo Camera", frame)

        if cv2.waitKey(1) & 0xFF == ord("q"):
            break

    cap.release()
    cv2.destroyAllWindows()
    return True

# ëŒ€í™” ì‹¤í–‰
def start_bangboo_conversation():
    user_id = "user123"  # ê³ ì • ì‚¬ìš©ì ID
    print("Bangboo: ì•ˆë…•í•˜ì„¸ìš”! ì €ëŠ” Bangbooì—ìš”. ë‹¹ì‹ ê³¼ ëŒ€í™”í•˜ê³  ì‹¶ì–´ìš”!")
    
    while True:
        user_input = input("You: ")
        if user_input.lower() in ["ë", "ì¢…ë£Œ", "bye"]:
            print("Bangboo: ëŒ€í™”í•´ì¤˜ì„œ ê³ ë§ˆì›Œìš”! ë‚˜ì¤‘ì— ë˜ ë§Œë‚˜ìš”! ğŸ¾")
            save_conversation_history(short_term_memory)  # ì¢…ë£Œ ì‹œ ëŒ€í™” ê¸°ë¡ ì €ì¥
            break
        reply = bangboo_reply(user_id, user_input)
        print(f"Bangboo: {reply}")

# í”„ë¡œê·¸ë¨ ì‹¤í–‰
initialize_files()

# ì¹´ë©”ë¼ ë¨¼ì € ì‹¤í–‰
camera_status = capture_camera()

# ëŒ€í™” ê¸°ëŠ¥ ì‹¤í–‰
start_bangboo_conversation()